I initially used the default parameters for the Decision Tree Classifier to estimate the accuracy of the model. accuracy was 0.77. Then, I used GridSearchCV to tune the hyperparameters of the model. The accuracy of the tuned model was 0.83. Among the ensemble methods, I used Bagging Classifier and AdaBoost. The Bagging Classifier performed better than AdaBoost (0.86 vs 0.77) . Next, Random Forest Classifier was used with RandomizedSearchCV to tune the hyperparameters. The achieved accuracy was 0.87. KNN was also used with GridSearchCV to tune the hyperparameters. The achieved accuracy was 0.86. I applied the non parametric method of K-Nearest Neighbor with exhustive to the data. The metric, manhattan, and weight, distance, were the best parameters. The accuracy of the model was 0.73. The Gaussian Naive Bayes with 1e-06 as the smoothing parameter showed promise with the accuracy of 0.85. The Multi-layer Perceptron Classifier was another model that I used. The accuracy of the model was 0.86. The best parameters were hidden_layer_sizes = (100, ), activation = 'logistic', solver = 'lbfgs', and 'alpha': 0.05. Finally, Support Vector Machine was used with GridSearchCV to tune the hyperparameters. The algorithm performed poorest among all the models with the accuracy of 0.51.
