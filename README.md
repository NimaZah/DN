# Diagnosis Alzheimer With handwriting Dataset
The 'Diagnosis Alzheimer With Handwriting' dataset is a unique collection of handwriting samples from patients with Alzheimer's disease and a control group. The data was collected from 174 individuals while performing handwriting activities according to a technique designed for the early identification of Alzheimer's disease. The dataset contains elements that aid in identifying Alzheimer's disease by capturing the unique aspects of handwriting. 
The use of graphics tablets for administering handwriting tests has shown promise in decreasing the time taken to perform clinical assessments, and machine learning methods can be used to record the kinematic and dynamic information of performed movements.
## Experiment with algorithms. The authors of this study used a tablet with a white template and a conventional ink pen to collect data from 37 people with Parkinson's disease and 38 normal people. We collected handwriting data from 174 participants, including 89 AD patients and 85 healthy people, and we tested their abilities in the following categories: copy, memory, dictation and copying.
We performed experiments to validate a feature set that can discriminate between AD patients and healthy people.
The results of the experiment show that all the classifiers performed well (mean accuracy >70%), and the differences between the best classifiers, i.e. RF, LR, GNB and MLP, were not statistically significant.
The aim of the work reported in this paper is to show that the analysis of handwriting is an innovative way of characterizing the effects of Alzheimer's disease.
Based on the combination of the five best performing SVM-based classifiers and the short time needed to execute the test, we believe that the combination of the best performance and time efficiency will eventually favor the adoption of this test by physicians.

I initially used the default parameters for the Decision Tree Classifier to estimate the model accuracy (which resulted in 0.77.)  I then used GridSearchCV to tune the hyperparameters of the model. The accuracy of the tuned model was 0.83. Among the ensemble methods, I used the Bagging classifier and AdaBoost. The Bagging Classifier performed better than AdaBoost (0.86 v.s 0.77) Afterwards, the Random Forest classifier was used with RandomizedSearchCV to tune the hyperparameters. The achieved accuracy was 0.87. KNN was also used with GridSearchCV to tune the hyperparameters with an accuracy of 0.86. I applied the nonparametric method of the K-Nearest Neighbor and performed an exhaustive search. The metric, 'manhattan,' and weight, 'distance,' were the best parameters. The accuracy of the model was 0.73. The Gaussian Naive Bayes with 1e-06 as the smoothing parameter showed promise with an accuracy of 0.85. The Multi-layer Perceptron Classifier was another model that I employed (accuracy 0.86.) The best parameters were hidden_layer_sizes = (100, ), activation = 'logistic', solver = 'lbfgs', and 'alpha': 0.05. Finally, the Support Vector Machine was used with GridSearchCV for hyperparameters tuning. The algorithm performed poorly compared to all other models with an accuracy of 0.51. I eventually used the Voting Classifier to combine the results with the best models, and arrived at an accuracy of 94.29%.
