I initially used the default parameters for the Decision Tree Classifier to estimate the accuracy of the model. accuracy was 0.77. Then, I used GridSearchCV to tune the hyperparameters of the model. The accuracy of the tuned model was 0.83. Among the ensemble methods, I used Bagging Classifier and AdaBoost. The Bagging Classifier performed better than AdaBoost (0.86 vs 0.77) . Next, Random Forest Classifier was used with RandomizedSearchCV to tune the hyperparameters. The achieved accuracy was 0.87. KNN was also used with GridSearchCV to tune the hyperparameters. The achieved accuracy was 0.86. Finally, I used Voting Classifier to combine the results of the tuned models. I applied the non parametric method of K-Nearest Neighbor to the data. The rationale behind the classifier is: p(ci | X) = 1/k * sum(p(cj | X)) where ci is the class of the test data and cj is the class of the k nearest neighbors.
