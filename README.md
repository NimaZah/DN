## Experiment with algorithms
I initially used the default parameters for the Decision Tree Classifier to estimate the model accuracy (which resulted in 0.77.)  I then used GridSearchCV to tune the hyperparameters of the model. The accuracy of the tuned model was 0.83. Among the ensemble methods, I used the Bagging classifier and AdaBoost. The Bagging Classifier performed better than AdaBoost (0.86 v.s 0.77) Afterwards, the Random Forest classifier was used with RandomizedSearchCV to tune the hyperparameters. The achieved accuracy was 0.87. KNN was also used with GridSearchCV to tune the hyperparameters with an accuracy of 0.86. I applied the nonparametric method of the K-Nearest Neighbor and performed an exhaustive search. The metric, 'manhattan,' and weight, 'distance,' were the best parameters. The accuracy of the model was 0.73. The Gaussian Naive Bayes with 1e-06 as the smoothing parameter showed promise with an accuracy of 0.85. The Multi-layer Perceptron Classifier was another model that I employed (accuracy 0.86.) The best parameters were hidden_layer_sizes = (100, ), activation = 'logistic', solver = 'lbfgs', and 'alpha': 0.05. Finally, the Support Vector Machine was used with GridSearchCV for hyperparameters tuning. The algorithm performed poorest among all the models with an accuracy of 0.51. Eventually, I used Voting Classifier to combine the results of the best models. Accuracy: 94.29%.
